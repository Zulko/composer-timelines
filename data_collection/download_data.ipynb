{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8ba556-dcb5-4285-8d57-c41aef60447d",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20375bc0-8467-42b6-85bb-e37b2c81ac04",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246fffa4-d6b2-4740-b160-cc0a1b07d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import bs4\n",
    "from fastprogress import fastprogress\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_KEY\"]\n",
    "\n",
    "def ask_chatgpt_with_pythonic_output(query, model=\"gpt-4o\", sleep_after=0, retries=0, expected_type=None):\n",
    "    for attempt in range(retries + 1):\n",
    "        system_prompt = (\n",
    "            \"Each of your answers should all be valid a python object \"\n",
    "            \"such as a list, dictionary, etc. \"\n",
    "            \"Answer with no introduction and no markdown formatting\"\n",
    "        )\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "        )\n",
    "        response = response['choices'][0]['message']['content']\n",
    "        try:\n",
    "            response = eval(response)\n",
    "            if expected_type is not None:\n",
    "                assert isinstance(response, expected_type)\n",
    "        except Exception as e:\n",
    "            if attempt < retries:\n",
    "                print (\"ChatGPT didn't produce valid python, trying again.\")\n",
    "                time.sleep(sleep_after)\n",
    "            else:\n",
    "                raise ValueError(f\"ChatGPT answer was not valid python: {response}\")\n",
    "            \n",
    "\n",
    "        time.sleep(sleep_after)\n",
    "        return response\n",
    "\n",
    "@lru_cache\n",
    "def download_page(url, filepath=None, sleep_after=0):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html = response.text\n",
    "        time.sleep(sleep_after)\n",
    "        if filepath is not None:\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(html)\n",
    "        return html\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def chunk_list(mylist, chunk_size, overlap=0):\n",
    "    return [mylist[i:i + chunk_size + overlap] for i in range(0, len(mylist), chunk_size)]\n",
    "        \n",
    "def save_to_json(data, filepath):\n",
    "    json.dump(data, open(filepath, \"w\"), indent=2)\n",
    "\n",
    "def load_from_json(filepath):\n",
    "    return json.load(open(filepath, \"r\"))\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "\n",
    "if not DATA_PATH.is_dir():\n",
    "    DATA_PATH.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c707f-9737-4f4a-ba58-81edc4d251e7",
   "metadata": {},
   "source": [
    "## Generate the composer list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6324ac42-b642-4896-b721-831f7a972edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 composers: Johann Sebastian Bach,Ludwig van Beethoven,Wolfgang Amadeus Mozart,Franz Schubert,Johannes Brahms,Pyotr Ilyich Tchaikovsky,Richard Wagner,Franz Liszt,Giuseppe Verdi,Antonio Vivaldi,Gioachino Rossini,Frédéric Chopin,Robert Schumann,Gustav Mahler,Felix Mendelssohn,Antonín Dvořák,Camille Saint-Saëns,Georges Bizet,Hector Berlioz,Jean Sibelius,Edvard Grieg,Claude Debussy,Sergei Rachmaninoff,Maurice Ravel,Alexander Scriabin,Igor Stravinsky,Dmitri Shostakovich,Modest Mussorgsky,Nikolai Rimsky-Korsakov,Mikhail Glinka,Carl Maria von Weber,Gaetano Donizetti,Vincenzo Bellini,Gustav Holst,Ralph Vaughan Williams,Ferruccio Busoni,Edward Elgar,Arthur Sullivan,Charles Gounod,Leoš Janáček,Franz Lehar,Paul Dukas,Ottorino Respighi,Alexander Borodin,Manuel de Falla,Heitor Villa-Lobos,Aaron Copland,Samuel Barber,Benjamin Britten,Béla Bartók,Zoltán Kodály,Arnold Schoenberg,Alban Berg,Anton Webern,George Gershwin,Scott Joplin,Erik Satie,César Franck,Jules Massenet,Jacques Offenbach,Jean-Philippe Rameau,Christoph Willibald Gluck,Muzio Clementi,John Field,Charles-Valentin Alkan,Isaac Albéniz,Enrique Granados,Nikolai Medtner,Sergei Prokofiev,Aram Khachaturian,Josef Suk,Bedřich Smetana,Josef Haydn,Luigi Boccherini,Ludwig Spohr,Carl Czerny,Francisco Tárrega,Leonard Bernstein,Gabriel Fauré,Edgar Varèse,Olivier Messiaen,Darius Milhaud,Francis Poulenc,Nadia Boulanger,Ruth Crawford Seeger,Henry Cowell,Kaikhosru Shapurji Sorabji,Frank Martin,Luciano Berio,Karlheinz Stockhausen,György Ligeti,Hans Werner Henze,Alfred Schnittke,Pierre Boulez,Iannis Xenakis,Astor Piazzolla,Carl Orff,Dmitry Kabalevsky,Krzysztof Penderecki,Alberto Ginastera,Samuel Coleridge-Taylor,Amy Beach,Vítězslava Kaprálová,Louise Farrenc,Lili Boulanger\n"
     ]
    }
   ],
   "source": [
    "composers_list = ask_chatgpt_with_pythonic_output(\n",
    "    \"\"\"Give me a list of the full names of the most famous classical composers.\n",
    "    Only include composers born after 1500 and dead before 1976.\n",
    "    Make sure the list has more than 100 composers.\n",
    "    \"\"\",\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "print (f\"{len(composers_list)} composers: {','.join(composers_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c1ebbd6-c4c4-4ed8-8d69-19fd98550728",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(composers_list, DATA_PATH / \"composers_list.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741ebfb-c01d-4aa8-a34d-516a51c826c0",
   "metadata": {},
   "source": [
    "## Add a bit of structure/metadata to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8667a329-7b5b-4606-b3f1-108dc44223b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_composer_details(composers):\n",
    "    return ask_chatgpt_with_pythonic_output(\"\"\"\n",
    "    For the following composers, add their birth and death years.\n",
    "    Return a list where elements have the following schema:\n",
    "    {\n",
    "      full_name: str,\n",
    "      first_names: str,\n",
    "      last_name: str,\n",
    "      birth_year: int,\n",
    "      death_year: int\n",
    "    }\n",
    "    \n",
    "    Composers:\n",
    "    \"\"\" + \",\".join(composers))\n",
    "\n",
    "composers_list = load_from_json(DATA_PATH / \"composers_list.json\")\n",
    "composers_with_metadata = [\n",
    "    data\n",
    "    for composer_chunk in tqdm(chunk_list(composers_list, 10))\n",
    "    for data in get_composer_details(composer_chunk)\n",
    "]\n",
    "save_to_json(composers_with_metadata, DATA_PATH / \"composer_list_with_metadata.json\")\n",
    "len(composers_with_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784213d-4a87-4553-b5ae-d3d0ad797564",
   "metadata": {},
   "source": [
    "## Get the wikipedia pages URLs of all composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7e8b8b-35f6-4494-93e1-3cc3984ce625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 105/105 [04:22<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_url(composer_name, sleep_after=2):\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": composer_name,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(search_url, params=params)\n",
    "    data = response.json()\n",
    "    time.sleep(sleep_after)\n",
    "    \n",
    "    if data['query']['search']:\n",
    "        title = data['query']['search'][0]['title']\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        return page_url\n",
    "    else:\n",
    "        print (f\"No page found for {composer_name}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "composers_list = load_from_json(DATA_PATH / \"composers_list.json\")\n",
    "composer_pages_urls = {\n",
    "    composer_name: get_wikipedia_url(composer_name)\n",
    "    for composer_name in tqdm(composers_list)\n",
    "}\n",
    "save_to_json(composer_pages_urls, DATA_PATH / \"composers_wikipedia_urls.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877cbbce-0b16-474e-b05b-1ddaee8db11e",
   "metadata": {},
   "source": [
    "## Download the composer wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed9a755-387a-4c75-b9ac-a57f92069d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 105/105 [00:00<00:00, 89114.11it/s]\n"
     ]
    }
   ],
   "source": [
    "composer_pages_urls = load_from_json(DATA_PATH / \"composers_wikipedia_urls.json\")\n",
    "pages_dir = DATA_PATH / \"composers_wikipedia_html\"\n",
    "if not pages_dir.is_dir():\n",
    "    pages_dir.mkdir()\n",
    "for composer_name, wikipedia_page in tqdm(composer_pages_urls.items()):\n",
    "    target_path = pages_dir / (composer_name + \".html\")\n",
    "    if not target_path.is_file():\n",
    "        download_page(wikipedia_page, target_path, sleep_after=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277ae46-f643-41d9-804c-5e805840b989",
   "metadata": {},
   "source": [
    "## Summarize the composer wikipedia pages\n",
    "\n",
    "In this separate pages in chunks of 2000 words and we enforce a 15s pause between the processing of chunk. As a result this takes hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661813c1-50ef-45f0-ad3b-b96600294c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT didn't produce valid python, trying again.\n",
      "ChatGPT didn't produce valid python, trying again.\n",
      "ChatGPT didn't produce valid python, trying again.\n",
      "ChatGPT didn't produce valid python, trying again.\n",
      "ChatGPT didn't produce valid python, trying again.\n",
      "ChatGPT didn't produce valid python, trying again.\n"
     ]
    }
   ],
   "source": [
    "prompt_template =\"\"\"\n",
    "From the biography below, return a list of the major events in the composer's life.\n",
    "Do not include actions by other people after the composer's death.\n",
    "The answer will be of the form [event_1, event_2, ...]\n",
    "where each event has the following schema\n",
    "{\"event\": str, \"summary\": str, \"year\": int, \"city\": str, \"country\": str}. \n",
    "The year should always be a single integer, and only an integer.\n",
    "\n",
    "The summary should be a few sentences describing the event and context.\n",
    "The summary should be very informative but also funny.\n",
    "Avoid dark humor and don't joke about tragedies.\n",
    "Don't mention the year. Stick to the events in the biography, do not invent.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_events(text):\n",
    "    return ask_chatgpt_with_pythonic_output(\n",
    "        prompt_template + text,\n",
    "        sleep_after=15,\n",
    "        model='gpt-4o-mini',\n",
    "        retries=1,\n",
    "        expected_type=list\n",
    "    )\n",
    "\n",
    "pages_dir = DATA_PATH / \"composers_wikipedia_html\"\n",
    "events_dir = DATA_PATH / \"composer_events\" \n",
    "if not events_dir.is_dir():\n",
    "    events_dir.mkdir()\n",
    "master_progress_bar = fastprogress.master_bar(list(pages_dir.glob(\"*.html\")))\n",
    "for page_path in master_progress_bar:\n",
    "    composer_name = page_path.name.split(\".\")[0]\n",
    "    target_file = events_dir / (composer_name + \".json\")\n",
    "    if target_file.is_file():\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "    html = page_path.read_text()\n",
    "    soup = bs4.BeautifulSoup(html)\n",
    "    biography = soup.select(\"#mw-content-text\")[0].get_text()\n",
    "    words = biography.split(\" \")\n",
    "    word_chunks = chunk_list(words, 2000, overlap=20)\n",
    "    events = [\n",
    "        event\n",
    "        for chunk in fastprogress.progress_bar(word_chunks, parent=master_progress_bar)\n",
    "        for event in get_events(\" \".join(chunk))\n",
    "    ]\n",
    "    save_to_json(events, target_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f346c-2199-4bfc-9462-7d2d997b1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template =\"\"\"\n",
    "Given the list of events below,\n",
    "detect entries which describe the exact same event (same place, same year, same story),\n",
    "and return a deduplicated copy of the list with redundancies removed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def deduplicate_events(events_list):\n",
    "    return ask_chatgpt_with_pythonic_output(\n",
    "        prompt_template + str(events_list),\n",
    "        model='gpt-4o-mini',\n",
    "        retries=1,\n",
    "        expected_type=list\n",
    "    )\n",
    "\n",
    "events_dir = DATA_PATH / \"composer_events\" \n",
    "deduplicated_events_dir =  DATA_PATH / \"deduplicated_events\" \n",
    "if not deduplicated_events_dir.is_dir():\n",
    "    deduplicated_events_dir.mkdir()\n",
    "master_progress_bar = fastprogress.progress_bar(list(events_dir.glob(\"*.json\")))\n",
    "for events_path in master_progress_bar:\n",
    "    composer_name = events_path.name.split(\".\")[0]\n",
    "    target_file = deduplicated_events_dir / (composer_name + \".json\")\n",
    "    if target_file.is_file():\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "    events_list = load_from_json(events_path)\n",
    "    deduplicated_events = deduplicate_events(events_list)\n",
    "    # print (composer_name, len(events_list), len(deduplicated_events))\n",
    "    save_to_json(deduplicated_events, target_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f06f6-cdde-4d91-aaa1-83f4500089c7",
   "metadata": {},
   "source": [
    "## Download the wikipedia pages for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076f352-7c1c-4f80-af28-0273f77fc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_wikipedia_html_dir = DATA_PATH / \"years_wikipedia_html\" \n",
    "if not year_events_pages.is_dir():\n",
    "    year_events_pages.mkdir()\n",
    "\n",
    "for year in tqdm(range(1500, 2000)):\n",
    "    target_file = years_wikipedia_html_dir / f\"{year}.html\"\n",
    "    if target_file.is_file():\n",
    "        continue\n",
    "    wikipedia_url = f\"https://en.wikipedia.org/wiki/{year}\"\n",
    "    download_page(wikipedia_url, target_file, sleep_after=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddaa791-e1b7-4112-857f-985280d2f8d9",
   "metadata": {},
   "source": [
    "## Get compositions from IMSLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3df1c4-6db2-484f-9300-fd833abdc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMSLP_URL = \"https://imslp.org\"\n",
    "\n",
    "def detect_imslp_link(html):\n",
    "    soup = bs4.BeautifulSoup(html)\n",
    "    for link in soup.select(\"a\"):\n",
    "        if \"href\" in link.attrs: \n",
    "            href = link.attrs[\"href\"]\n",
    "            if href.startswith(IMSLP_URL + \"/wiki/Category\"):\n",
    "                return href\n",
    "\n",
    "def detect_year(txt):\n",
    "    # Regular expression to find all numbers in the text\n",
    "    numbers = re.findall(r'\\b\\d{4}\\b', txt)\n",
    "    \n",
    "    for num in numbers:\n",
    "        year = int(num)\n",
    "        if 1000 <= year <= 3000:\n",
    "            return year\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_publication_year(work_url):\n",
    "    work_html = download_page(work_url, sleep_after=0)\n",
    "    try:\n",
    "        soup = bs4.BeautifulSoup(work_html)\n",
    "    except:\n",
    "        return None\n",
    "    indicators = [\"First Publication\", \"Composition Year\"]\n",
    "    trs = [\n",
    "        tr for tr in soup.select(\"tr\")\n",
    "        if any([indicator in tr.get_text() for indicator in indicators])\n",
    "    ]\n",
    "    if trs == []:\n",
    "        return None\n",
    "    year = trs[0].select(\"td\")[0].get_text()\n",
    "    return detect_year(year)\n",
    "    \n",
    "\n",
    "pages_dir = DATA_PATH / \"composers_wikipedia_html\"\n",
    "compositions_dir = DATA_PATH / \"compositions\"\n",
    "if not (compositions_dir.is_dir()):\n",
    "    compositions_dir.mkdir()\n",
    "progress_master_bar = master_bar(list(pages_dir.glob(\"*.html\"))) \n",
    "for page_path in progress_master_bar:\n",
    "    composer_name = page_path.name.split(\".\")[0]\n",
    "    target_file = compositions_dir / (composer_name + \".json\")\n",
    "    if target_file.is_file():\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "    wikipedia_html = page_path.read_text()\n",
    "    imslp_url = detect_imslp_link(wikipedia_html)\n",
    "    if imslp_url is None:\n",
    "        continue\n",
    "    \n",
    "    imslp_composer = imslp_url.split(\"Category:\")[1].replace(\"_\", \" \")\n",
    "    \n",
    "    imslp_html = download_page(imslp_url)\n",
    "    soup = bs4.BeautifulSoup(imslp_html)\n",
    "    compositions = []\n",
    "    links_to_composer_works = [\n",
    "        a for section in soup.select(\"#mw-pages\")\n",
    "        for a in section.find_all(\"a\", class_=\"categorypagelink\")\n",
    "        if imslp_composer.replace(\" \", \"_\") in a.attrs[\"href\"]\n",
    "    ]\n",
    "    for link in progress_bar(links_to_composer_works, parent=progress_master_bar):\n",
    "        piece_title = link.text.split(\"(\")[0].strip()\n",
    "        work_url = IMSLP_URL + link.attrs[\"href\"].replace(\" \", \"_\")\n",
    "        year = get_publication_year(work_url)\n",
    "        compositions.append({\n",
    "            \"imslp_url\": work_url,\n",
    "            \"year\": year,\n",
    "            \"title\": piece_title\n",
    "        })\n",
    "    save_to_json(compositions, target_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761a5c5-fb5c-4eb4-9556-825507c026a2",
   "metadata": {},
   "source": [
    "## Compile composer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e0c1fb-00b0-4a0e-ae82-5c4083266868",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_list = load_from_json(DATA_PATH / \"composer_list_with_metadata.json\")\n",
    "wikipedia_urls = load_from_json(DATA_PATH / \"composers_wikipedia_urls.json\")\n",
    "\n",
    "full_composer_data_dir = DATA_PATH / \"full_composer_data\"\n",
    "if not full_composer_data_dir.is_dir():\n",
    "    full_composer_data_dir.mkdir()\n",
    "\n",
    "for composer in composers_list:\n",
    "    full_name = composer[\"full_name\"]\n",
    "    events_json = DATA_PATH / \"deduplicated_events\" / f\"{full_name}.json\"\n",
    "    if not events_json.is_file():\n",
    "        # print (f\"No events for {full_name}\")\n",
    "        continue\n",
    "    compositions_json = DATA_PATH / \"compositions\" / f\"{full_name}.json\"\n",
    "    if not compositions_json.is_file():\n",
    "        # print (f\"No compositions for {full_name}\")\n",
    "        continue\n",
    "    full_composer_data = {**composer}\n",
    "    full_composer_data[\"wikipedia_url\"] = wikipedia_urls[full_name]\n",
    "    full_composer_data[\"events\"] = load_from_json(events_json)\n",
    "    full_composer_data[\"compositions\"] = load_from_json(compositions_json)\n",
    "    save_to_json(full_composer_data, full_composer_data_dir / f\"{full_name}.json\")\n",
    "    # print (composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e6786-677e-43b6-b9c1-27e9843fc281",
   "metadata": {},
   "source": [
    "## Summarize the events of all years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdb707-3372-4378-b936-0058e3f6e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "From the text below, select the top ~10 major events.\n",
    "Prefer major technical advances, or major events which\n",
    "would have made the front page of European newspapers.\n",
    "Prefer events which could have had an impact on citizens\n",
    "and in particular music composers.\n",
    "\n",
    "Return a list of the form [event_1, event_2, ...]\n",
    "where each event has the following schema\n",
    "{\"event\": str, \"summary\": str, \"year\": int, \"city\": str, \"country\": str}\n",
    "The year should always be a single integer, and only an integer.\n",
    "Never use quotation marks inside the summary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_world_events(text):\n",
    "    prompt = prompt_template + text\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return ask_chatgpt_with_pythonic_output(prompt, sleep_after=1, model='gpt-4o-mini')\n",
    "        except Exception as err:\n",
    "            if attempt == 2:\n",
    "                raise (err)\n",
    "\n",
    "def extract_events_text(html):\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    events_section = soup.find('h2', string=\"Events\")\n",
    "    \n",
    "    if not events_section:\n",
    "        return None\n",
    "    \n",
    "    # Extract all text between the \"Events\" h2 and the next h2\n",
    "    events_text = []\n",
    "    for sibling in events_section.parent.find_next_siblings():\n",
    "        children = list(sibling.children)\n",
    "        if len(children) and children[0].name == 'h2':\n",
    "            break\n",
    "        events_text.append(sibling.get_text())\n",
    "    \n",
    "    return '\\n'.join(events_text)\n",
    "                \n",
    "years_wikipedia_html_dir = DATA_PATH / \"years_wikipedia_html\" \n",
    "year_world_events_dir = DATA_PATH / \"year_world_events\" \n",
    "if not word_events_dir.is_dir():\n",
    "    word_events_dir.mkdir()\n",
    "\n",
    "for page_path in tqdm(list(years_wikipedia_html_dir.glob(\"*.html\"))):\n",
    "    target_file = year_world_events_dir / (f\"{page_path.name.replace('.html', '')}.json\")\n",
    "    if target_file.is_file():\n",
    "        continue\n",
    "    html = page_path.read_text()\n",
    "    events_text = extract_events_text(html)\n",
    "    selected_events_list = get_world_events(events_text)\n",
    "    save_to_json(selected_events_list, target_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9aec3-c533-4220-aa33-bc63c0f26264",
   "metadata": {},
   "source": [
    "### Compile world events to a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82590fbf-8c6e-4643-b33b-0cdc133e96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_world_events = {\n",
    "    year: load_from_json(DATA_PATH / \"year_world_events\" / f\"{year}.json\")\n",
    "    for year in range(1500, 2000)\n",
    "}\n",
    "save_to_json(year_world_events, DATA_PATH / \"year_world_events.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
